{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "tpu-jigsaw-multilingual-toxic-colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martin-fabbri/jigsaw-multilingual-toxic-comment/blob/main/notebooks/tpu_jigsaw_multilingual_toxic_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohoJzR1RO7eL"
      },
      "source": [
        "# Jigsaw Multilingual Toxic Comment Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i45WcRaPN7Vj"
      },
      "source": [
        "%%capture\r\n",
        "!pip install fsspec\r\n",
        "!pip install gcsfs\r\n",
        "!pip install --upgrade --pre dvc\r\n",
        "!git clone https://github.com/martin-fabbri/jigsaw-multilingual-toxic-comment.git\r\n",
        "%cd /content/jigsaw-multilingual-toxic-comment/"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSF7sEIAUpxR",
        "outputId": "3eae8da2-f58c-435b-a134-e6931d7ccfb6"
      },
      "source": [
        "!pip list | grep dvc"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dvc                           2.0.0a2       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLEZhnw1O2xY",
        "trusted": true
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras import Model\n",
        "import tensorflow_hub as hub\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPYLOws9U4Ku"
      },
      "source": [
        "## Local dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS68Cr0UU320"
      },
      "source": [
        "%%capture\r\n",
        "!dvc pull -r origin data/raw/jigsaw-toxic-comment-train-processed-seqlen128.csv\r\n",
        "!dvc pull -r origin data/raw/validation-processed-seqlen128.csv"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFFXxEYr0IxH"
      },
      "source": [
        "## TPU setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8D2-qgGtCB8",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b290fdd-26e3-4629-cb42-80b3f3e46158"
      },
      "source": [
        "%%capture\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "print(\"Number of accelerators:\", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.60.38.106:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.60.38.106:8470\n",
            "INFO:tensorflow:Clearing out eager caches\n",
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCduuDXVtdAZ"
      },
      "source": [
        "## Load config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSxMcLhRed-w",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e224aacd-a54f-4d24-e252-79e78a10a91f"
      },
      "source": [
        "SEQUENCE_LENGTH = 128\n",
        "EPOCHS = 6\n",
        "GCS_PATH = \"gs://kds-d6b459191750de20505baf9adc31878a65fd287afd0812a8deb1cb15/\"\n",
        "TRAIN_PREFIX = \"jigsaw-toxic-comment-train-processed-seqlen\"\n",
        "TRAIN_PROCESSED = f\"{GCS_PATH}{TRAIN_PREFIX}{SEQUENCE_LENGTH}.csv\"\n",
        "#TRAIN_PROCESSED = \"data/raw/jigsaw-toxic-comment-train-processed-seqlen128.csv\"\n",
        "\n",
        "VALID_PREFIX = \"validation-processed-seqlen\"\n",
        "VALID_DATA = f\"{GCS_PATH}{VALID_PREFIX}{SEQUENCE_LENGTH}.csv\"\n",
        "\n",
        "BERT_GCS_PATH = \"gs://bert_multilingual_public/bert_multi_cased_L-12_H-768_A-12_2/\"\n",
        "BATCH_SIZE = 128 * strategy.num_replicas_in_sync\n",
        "LR_MAX = 0.001 * strategy.num_replicas_in_sync\n",
        "LR_EXP_DECAY = .9\n",
        "LR_MIN = 0.0001\n",
        "TRAIN_DATA_LENGTH = 223549  # count_dataset_steps(TRAIN_PROCESSED) = 223549 \n",
        "STEPS_PER_EPOCH = TRAIN_DATA_LENGTH // BATCH_SIZE\n",
        "\n",
        "print(f\"EPOCHS:            {EPOCHS:,}\")\n",
        "print(f\"BATCH_SIZE:        {BATCH_SIZE:,}\")\n",
        "print(f\"STEPS_PER_EPOCH:   {STEPS_PER_EPOCH:,}\")\n",
        "print(f\"TRAIN_DATA_LENGTH: {TRAIN_DATA_LENGTH:,}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCHS:            6\n",
            "BATCH_SIZE:        1,024\n",
            "STEPS_PER_EPOCH:   218\n",
            "TRAIN_DATA_LENGTH: 223,549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd3A4A6w0XG_"
      },
      "source": [
        "## Explore dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "931ccmkH0alI",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "7a43ac81-44dd-4c2c-9e67-18d3faef976d"
      },
      "source": [
        "%load_ext google.colab.data_table\n",
        "comments = pd.read_csv(TRAIN_PROCESSED)\n",
        "comments.head(2)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The google.colab.data_table extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.colab.data_table\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.module+javascript": "\n      import \"https://ssl.gstatic.com/colaboratory/data_table/a6224c040fa35dcf/data_table.js\";\n\n      window.createDataTable({\n        data: [[{\n            'v': 0,\n            'f': \"0\",\n        },\n\"0000997932d777bf\",\n\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n\"(101, 27746, 31609, 11809, 24781, 10105, 70971, 10107, 11019, 10571, 15127, 29115, 23920, 72832, 56684, 30126, 10309, 86095, 46949, 136, 11696, 10309, 10115, 112, 188, 109995, 33269, 12387, 117, 12820, 69177, 10135, 11152, 74212, 10107, 10662, 146, 34584, 10160, 10287, 10482, 80332, 20794, 10858, 119, 12689, 20648, 42658, 10112, 16938, 112, 188, 51600, 10105, 101986, 23953, 10188, 10105, 31311, 15975, 11764, 146, 112, 181, 18675, 11858, 119, 12642, 119, 20862, 119, 11171, 119, 10365, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\",\n\"(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\",\n\"(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\"],\n [{\n            'v': 1,\n            'f': \"1\",\n        },\n\"000103f0d9cfb60f\",\n\"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\",\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n{\n            'v': 0,\n            'f': \"0\",\n        },\n\"(101, 141, 112, 56237, 10874, 106, 10357, 18258, 10531, 25903, 43361, 146, 112, 181, 57047, 56299, 28780, 31746, 10169, 119, 91327, 10107, 119, 113, 31311, 114, 10296, 131, 11524, 117, 11238, 10193, 117, 10255, 113, 11780, 114, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\",\n\"(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\",\n\"(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\"]],\n        columns: [[\"number\", \"index\"], [\"string\", \"id\"], [\"string\", \"comment_text\"], [\"number\", \"toxic\"], [\"number\", \"severe_toxic\"], [\"number\", \"obscene\"], [\"number\", \"threat\"], [\"number\", \"insult\"], [\"number\", \"identity_hate\"], [\"string\", \"input_word_ids\"], [\"string\", \"input_mask\"], [\"string\", \"all_segment_id\"]],\n        columnOptions: [{\"width\": \"1px\", \"className\": \"index_column\"}],\n        rowsPerPage: 25,\n        helpUrl: \"https://colab.research.google.com/notebooks/data_table.ipynb\",\n        suppressOutputScrolling: true,\n        minimumWidth: undefined,\n      });\n    ",
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>input_word_ids</th>\n",
              "      <th>input_mask</th>\n",
              "      <th>all_segment_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(101, 27746, 31609, 11809, 24781, 10105, 70971...</td>\n",
              "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>(101, 141, 112, 56237, 10874, 106, 10357, 1825...</td>\n",
              "      <td>(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ...                                     all_segment_id\n",
              "0  0000997932d777bf  ...  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1  000103f0d9cfb60f  ...  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "\n",
              "[2 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvZnikwFqFDc",
        "trusted": true
      },
      "source": [
        "def format_sentences(data, label=\"toxic\", remove_language=False):\n",
        "    labels = {\"labels\": data.pop(label)}\n",
        "    if remove_language:\n",
        "        languages = {\"language\": data.pop(\"lang\")}\n",
        "    for k, v in data.items():\n",
        "        data[k] = parse_string_list_into_ints(v)\n",
        "    return data, labels"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNkNnzoB5RJZ",
        "trusted": true
      },
      "source": [
        "def make_sentence_dataset_from_csv(\n",
        "    filename, label=\"toxic\", language_to_filter=None\n",
        "):\n",
        "    # This assumes the column order label, input_word_ids, input_mask, segment_ids\n",
        "    SELECTED_COLUMNS = [label, \"input_word_ids\", \"input_mask\", \"all_segment_id\"]\n",
        "    label_default = tf.int32 if label == \"id\" else tf.float32\n",
        "    COLUMN_DEFAULTS = [label_default, tf.string, tf.string, tf.string]\n",
        "\n",
        "    if language_to_filter:\n",
        "        insert_pos = 0 if label != \"id\" else 1\n",
        "        SELECTED_COLUMNS.insert(insert_pos, \"lang\")\n",
        "        COLUMN_DEFAULTS.insert(insert_pos, tf.string)\n",
        "    preprocessed_sentences_dataset = tf.data.experimental.make_csv_dataset(\n",
        "        filename,\n",
        "        column_defaults=COLUMN_DEFAULTS,\n",
        "        select_columns=SELECTED_COLUMNS,\n",
        "        batch_size=1,\n",
        "        num_epochs=1,\n",
        "        shuffle=False,\n",
        "    )  # We'll do repeating and shuffling ourselves\n",
        "    # make_csv_dataset required a batch size, but we want to batch later\n",
        "    preprocessed_sentences_dataset = preprocessed_sentences_dataset.unbatch()\n",
        "\n",
        "    if language_to_filter:\n",
        "        preprocessed_sentences_dataset = preprocessed_sentences_dataset.filter(\n",
        "            lambda data: tf.math.equal(\n",
        "                data[\"lang\"], tf.constant(language_to_filter)\n",
        "            )\n",
        "        )\n",
        "        # preprocessed_sentences.pop('lang')\n",
        "    preprocessed_sentences_dataset = preprocessed_sentences_dataset.map(\n",
        "        lambda data: format_sentences(\n",
        "            data, label=label, remove_language=language_to_filter\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return preprocessed_sentences_dataset\n",
        "   "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvvA5cLm-YP6",
        "trusted": true
      },
      "source": [
        "def parse_string_list_into_ints(strlist):\n",
        "    s = tf.strings.strip(strlist)\n",
        "    s = tf.strings.substr(\n",
        "        strlist, 1, tf.strings.length(s) - 2)  # Remove parentheses around list\n",
        "    s = tf.strings.split(s, ',', maxsplit=SEQUENCE_LENGTH)\n",
        "    s = tf.strings.to_number(s, tf.int32)\n",
        "    s = tf.reshape(s, [SEQUENCE_LENGTH])  # Force shape here needed for XLA compilation (TPU)\n",
        "    return s"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgAcfubB-Wo5",
        "trusted": true
      },
      "source": [
        "def format_sentences(data, label='toxic', remove_language=False):\n",
        "    labels = {'labels': data.pop(label)}\n",
        "    if remove_language:\n",
        "        languages = {'language': data.pop('lang')}\n",
        "    # The remaining three items in the dict parsed from the CSV are lists of integers\n",
        "    for k,v in data.items():  # \"input_word_ids\", \"input_mask\", \"all_segment_id\"\n",
        "        data[k] = parse_string_list_into_ints(v)\n",
        "    return data, labels"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69bMAIJZ-Nn1",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "7a420689-74e4-494e-8693-56234bfbd810"
      },
      "source": [
        "ds = make_sentence_dataset_from_csv(TRAIN_PROCESSED)\n",
        "X, y = next(iter(ds.take(1)))\n",
        "for k, v in X.items():\n",
        "    print(f\"--- {k} ---\")\n",
        "    print(v.numpy())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-f93343edd7e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_sentence_dataset_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PROCESSED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- {k} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: File system scheme '[local]' not implemented (file: 'data/raw/jigsaw-toxic-comment-train-processed-seqlen128.csv')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dGpGeNnA5Yy",
        "trusted": true
      },
      "source": [
        "y[\"labels\"].numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WptHAGLMJfgm",
        "trusted": true
      },
      "source": [
        "def count_dataset_steps(dataset):\n",
        "    # to be used on small datasets only: iterates through entire dataset and counts\n",
        "    cnt = 0\n",
        "    for data in dataset:\n",
        "        cnt += 1\n",
        "    return cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcCf7RaoJ4RA",
        "trusted": true
      },
      "source": [
        "# count_dataset_steps(ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bXqb2GbRLAf",
        "trusted": true
      },
      "source": [
        "def make_dataset_pipeline(dataset, repeat_and_shuffle=True):\n",
        "    cached_dataset = dataset.cache()\n",
        "    if repeat_and_shuffle:\n",
        "        cached_dataset = cached_dataset.repeat().shuffle(2048)\n",
        "        cached_dataset = cached_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    else:\n",
        "        cached_dataset = cached_dataset.batch(BATCH_SIZE)\n",
        "    cached_dataset = cached_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return cached_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISuJ1BXoRK91",
        "trusted": true
      },
      "source": [
        "english_train_dataset = make_dataset_pipeline(\n",
        "    make_sentence_dataset_from_csv(TRAIN_PROCESSED)\n",
        ")\n",
        "english_train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjpTmFO7RK69",
        "trusted": true
      },
      "source": [
        "non_english_val_datasets = {}\n",
        "non_english_val_datasets_steps = {}\n",
        "for language_name, language_label in [(\"Spanish\", \"es\"), ('Italian', 'it')]:\n",
        "    non_english_val_datasets[language_name] = make_sentence_dataset_from_csv(\n",
        "        VALID_DATA, language_to_filter=language_label\n",
        "    )\n",
        "    non_english_val_datasets[language_name] = make_dataset_pipeline(\n",
        "        non_english_val_datasets[language_name], repeat_and_shuffle=False\n",
        "    )\n",
        "    non_english_val_datasets_steps[language_name] = count_dataset_steps(\n",
        "        non_english_val_datasets[language_name]\n",
        "    )\n",
        "\n",
        "non_english_val_datasets[\"Combined\"] = make_sentence_dataset_from_csv(\n",
        "    VALID_DATA\n",
        ")\n",
        "non_english_val_datasets[\"Combined\"] = make_dataset_pipeline(\n",
        "    non_english_val_datasets[\"Combined\"], repeat_and_shuffle=False\n",
        ")\n",
        "non_english_val_datasets_steps[\"Combined\"] =  count_dataset_steps(\n",
        "    non_english_val_datasets[\"Combined\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzPg_iUvjtf1"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKjiiUc7j5Uq",
        "trusted": true
      },
      "source": [
        "def multilingual_bert_model(max_seq_lenght=SEQUENCE_LENGTH):\n",
        "    \"\"\"Build and return a multilingual BERT model and tokenizer.\"\"\"\n",
        "    input_word_ids = Input(\n",
        "        shape=(max_seq_lenght,), dtype=tf.int32, name=\"input_word_ids\"\n",
        "    )\n",
        "    input_mask = Input(\n",
        "        shape=(max_seq_lenght,), dtype=tf.int32, name=\"input_mask\"\n",
        "    )\n",
        "    segment_ids = Input(\n",
        "        shape=(max_seq_lenght,), dtype=tf.int32, name=\"all_segment_id\"\n",
        "    )\n",
        "\n",
        "    bert_layer = tf.saved_model.load(BERT_GCS_PATH)\n",
        "    bert_layer = hub.KerasLayer(bert_layer, trainable=True)\n",
        "\n",
        "    pooled_output, _ = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    output = Dense(32, activation=\"relu\")(pooled_output)\n",
        "    output = Dense(1, activation=\"sigmoid\", name=\"labels\", dtype=tf.float32)(\n",
        "        output\n",
        "    )\n",
        "\n",
        "    return Model(\n",
        "        inputs={\n",
        "            \"input_word_ids\": input_word_ids,\n",
        "            \"input_mask\": input_mask,\n",
        "            \"all_segment_id\": segment_ids,\n",
        "        },\n",
        "        outputs=output,\n",
        "    )"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k-6gAp1ZOPY",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355b3d33-1822-4d40-eed6-44d28d1d1428"
      },
      "source": [
        "with strategy.scope():\n",
        "    multilingual_bert = multilingual_bert_model()\n",
        "\n",
        "    multilingual_bert.compile(\n",
        "        loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001 * strategy.num_replicas_in_sync),\n",
        "        metrics = [tf.keras.metrics.AUC()],\n",
        "        steps_per_execution=16\n",
        "    )\n",
        "\n",
        "multilingual_bert.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "all_segment_id (InputLayer)     [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 177853441   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 all_segment_id[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 32)           24608       keras_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "labels (Dense)                  (None, 1)            33          dense[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 177,878,082\n",
            "Trainable params: 177,878,081\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOk46NvDuQiT",
        "trusted": true
      },
      "source": [
        "def lr_fn(epoch):\n",
        "    lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch) + LR_MIN\n",
        "    return lr"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evHJeQXkt78S",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57986ede-34b1-43ac-e5da-19b8641b8c78"
      },
      "source": [
        "# Train on English Wikipedia comment data.\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_fn)\n",
        "history = multilingual_bert.fit(\n",
        "    english_train_dataset,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    epochs=EPOCHS,\n",
        "    # validation_data=non_english_val_datasets[\"Combined\"],\n",
        "    # validation_steps=non_english_val_datasets_steps[\"Combined\"],\n",
        "    callbacks=[lr_callback],\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"SGD/gradients/StatefulPartitionedCall:1\", shape=(None,), dtype=int32), values=Tensor(\"SGD/gradients/StatefulPartitionedCall:0\", dtype=float32), dense_shape=Tensor(\"SGD/gradients/StatefulPartitionedCall:2\", shape=(None,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:437: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"while/SGD/gradients/StatefulPartitionedCall:1\", shape=(None,), dtype=int32), values=Tensor(\"while/SGD/gradients/StatefulPartitionedCall:0\", dtype=float32), dense_shape=Tensor(\"while/SGD/gradients/StatefulPartitionedCall:2\", shape=(None,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "218/218 [==============================] - 148s 679ms/step - loss: 0.2256 - auc: 0.8572\n",
            "Epoch 2/6\n",
            "218/218 [==============================] - 110s 504ms/step - loss: 0.1378 - auc: 0.9557\n",
            "Epoch 3/6\n",
            "218/218 [==============================] - 110s 504ms/step - loss: 0.1201 - auc: 0.9673\n",
            "Epoch 4/6\n",
            "218/218 [==============================] - 110s 504ms/step - loss: 0.1122 - auc: 0.9710\n",
            "Epoch 5/6\n",
            "218/218 [==============================] - 110s 504ms/step - loss: 0.1074 - auc: 0.9738\n",
            "Epoch 6/6\n",
            "218/218 [==============================] - 110s 504ms/step - loss: 0.1040 - auc: 0.9751\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}